# ğŸš— CarDekho Dataset - Used Car Price Prediction

This project focuses on predicting the **selling price of used cars** using various machine learning algorithms. By analyzing features like brand, manufacturing year, fuel type, and more, we aim to estimate fair market prices for vehicles in the Indian market. ğŸ‡®ğŸ‡³

---

## ğŸ“– About the Dataset

The dataset is sourced from **Kaggle** and contains detailed information about used cars listed on the CarDekho platform. Each record includes:

- Car brand and model  
- Year of manufacture  
- Fuel type (Petrol, Diesel, etc.)
- Seller type (Dealer or Individual)
- Transmission type  
- Number of kilometers driven  
- Ownership status  
- Selling price (target variable)  

This dataset is ideal for practicing **regression techniques** and understanding how different features influence used car prices.

---

## ğŸ¯ Objective

Build and evaluate multiple machine learning models to **predict the selling price** of used cars as accurately as possible.

---

## ğŸ› ï¸ What I Did

- âœ… Data cleaning and preprocessing  
- ğŸ“Š Exploratory Data Analysis (EDA)  
- ğŸ·ï¸ Encoded categorical variables  
- âš–ï¸ Handled missing values and outliers  
- ğŸ“ˆ Applied multiple regression algorithms:
  - ğŸ“ Linear Regression  
  - ğŸ”µ Ridge Regression (L2 Regularization)  
  - ğŸ”´ Lasso Regression (L1 Regularization)  
  - ğŸ“ K-Nearest Neighbors (KNN)  
  - ğŸŒ³ Decision Tree Regressor  
  - ğŸŒ² Random Forest Regressor  
  - ğŸš€ Gradient Boosting Regressor  
  - âš¡ **AdaBoost Regressor**  
  - ğŸ’¥ **XGBoost Regressor**

---

## ğŸ† Results

- ğŸ’¯ **XGBoost Regressor** delivered the **best performance** with an **RÂ² score of ~94%**, thanks to its ability to handle non-linearity, regularization, and ensemble learning.
- ğŸŒ² **Random Forest Regressor** also performed well, striking a strong balance between **bias and variance**.
- ğŸš€ **Gradient Boosting** was competitive, gradually improving residuals over iterations.
- âš¡ **AdaBoost** showed moderate performance but fell behind the other ensemble models.
- ğŸ”µğŸ”´ **Ridge and Lasso** (regularized linear models) helped control overfitting but were less powerful compared to tree-based models.
- ğŸ“ **KNN** and **Linear Regression** underperformed due to their limitations in capturing non-linear relationships.

---

## ğŸ“Œ Key Learnings

- Tree-based ensemble models are generally more effective for structured/tabular data.
- Boosting techniques (like XGBoost and Gradient Boosting) often outperform bagging models (like Random Forest) when properly tuned.
- Regularization techniques (Ridge, Lasso) are useful for avoiding overfitting in simpler models.
- **Hyperparameter tuning and feature engineering** play a crucial role in improving model performance.
