# ğŸš— CarDekho Dataset - Price Prediction

This project focuses on predicting the **selling price of used cars** using various machine learning algorithms. By analyzing car features such as brand, year, fuel type, and more, we aim to estimate fair prices for vehicles in the Indian market. ğŸ‡®ğŸ‡³

---

## ğŸ“– About the Dataset

The dataset is sourced from **Kaggle** and contains detailed information about used cars listed on the CarDekho platform. Each record includes:

- Car brand and model  
- Year of manufacture  
- Fuel type (Petrol, Diesel, etc.)  
- Seller type (Dealer or Individual)  
- Transmission type  
- Number of kilometers driven  
- Ownership status  
- Selling price (target variable)  

This dataset is ideal for practicing **regression techniques** and understanding how different features affect used car prices.

---

## ğŸ¯ Objective

Build and evaluate multiple machine learning models to **predict the selling price** of used cars as accurately as possible.

---

## ğŸ› ï¸ What I Did

- âœ… Data cleaning and preprocessing  
- ğŸ“Š Performed Exploratory Data Analysis (EDA)  
- ğŸ·ï¸ Encoded categorical variables  
- âš–ï¸ Handled missing values and outliers  
- ğŸ“ˆ Applied multiple regression algorithms:
  - ğŸ“ Linear Regression  
  - ğŸ”µ Ridge Regression (L2 Regularization)  
  - ğŸ”´ Lasso Regression (L1 Regularization)  
  - ğŸ“ K-Nearest Neighbors (KNN)  
  - ğŸŒ³ Decision Tree Regressor  
  - ğŸŒ² Random Forest Regressor  
  - ğŸš€ Gradient Boosting Regressor  
  - âš¡ **AdaBoost Regressor**  
  - ğŸ’¥ **XGBoost Regressor**

---

## ğŸ† Results

- âœ… **XGBoost Regressor** gave the **best performance** with approximately **94% accuracy** (RÂ² score), thanks to its ability to handle non-linearity, regularization, and boosting power.
- ğŸŒ² **Random Forest** was also strong, striking a great balance between **bias and variance**.
- ğŸš€ **Gradient Boosting** closely followed, performing well by improving residuals iteratively.
- âš¡ **AdaBoost** showed decent accuracy but underperformed compared to Random Forest and XGBoost.
- ğŸ§© Regularized models like Ridge and Lasso helped reduce overfitting but werenâ€™t top performers.
- ğŸ“ KNN and Linear Regression were the weakest due to their limitations in capturing non-linearities.

---

## ğŸ“Œ Key Learnings

- Tree-based ensemble models are generally more powerful for structured tabular data.
- Boosting algorithms (like XGBoost and Gradient Boosting) can outperform bagging (like Random Forest) with proper tuning.
- Regularization (Ridge, Lasso) is helpful but may not always outperform more complex models.
- Feature engineering and EDA significantly affect model performance.

---

## ğŸ“‚ Folder Structure

